{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import os \n",
    "import sys\n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(\"..\")  # Adjust if needed\n",
    "\n",
    "# Add the project root to sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from proteinshake.datasets import ProteinLigandInterfaceDataset, AlphaFoldDataset\n",
    "from src.utils import data_utils as dtu\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from src.models.basicVae import LitBasicVae\n",
    "import numpy as np\n",
    "from src.datasetClasses.sequenceDataset import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading AlphaFoldDataset_swissprot.residue.avro.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 356k/356k [04:20<00:00, 14.4MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting: 100%|██████████| 541143/541143 [07:05<00:00, 1271.75it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = AlphaFoldDataset(root='../data').to_point().torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4642"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial testing to be done with proteins less than or equal to 150 residues in length\n",
    "max_seq_length = dtu.get_max_seq_len(dataset)\n",
    "seq_lengths = dtu.get_dataset_seq_lengths(dataset,leq = max_seq_length)\n",
    "total_num = sum(seq_lengths.values())\n",
    "total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(x, x_hat,x_mu, x_logvar):\n",
    "\n",
    "    rec_loss =  torch.nn.functional.mse_loss(x_hat, x, reduction='sum')\n",
    "    KL_loss = -0.5 * torch.sum(1 + x_logvar - x_mu.pow(2) - x_logvar.exp())\n",
    "\n",
    "    return (rec_loss + KL_loss) / x.size(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "batch_size = 256\n",
    "seq_test_data = SequenceDataset(dataset)\n",
    "seq_dataloader = DataLoader(seq_test_data, batch_size=batch_size, shuffle=False)\n",
    "x_dim = seq_test_data[0].shape[0]\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicVae(latent_dim, x_dim, device).to(device)\n",
    "model_optim = torch.optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 8805.857168537297\n",
      "Epoch 1 Loss: 573.4133238073898\n",
      "Epoch 2 Loss: 486.7336163455493\n",
      "Epoch 3 Loss: 448.12906897557923\n",
      "Epoch 4 Loss: 427.8821528186537\n",
      "Epoch 5 Loss: 425.5512349899501\n",
      "Epoch 6 Loss: 1031.3734661781625\n",
      "Epoch 7 Loss: 448.97540769185105\n",
      "Epoch 8 Loss: 407.24881456976067\n",
      "Epoch 9 Loss: 514.6150695591757\n",
      "Epoch 10 Loss: 382.69239195732223\n",
      "Epoch 11 Loss: 367.11851961318763\n",
      "Epoch 12 Loss: 368.1743119644792\n",
      "Epoch 13 Loss: 353.2147185443199\n",
      "Epoch 14 Loss: 339.25786250911347\n",
      "Epoch 15 Loss: 376.5941721249933\n",
      "Epoch 16 Loss: 332.3824231918544\n",
      "Epoch 17 Loss: 336.3386949513056\n",
      "Epoch 18 Loss: 298.5822531817711\n",
      "Epoch 19 Loss: 282.57972617998513\n",
      "Epoch 20 Loss: 281.8378340838707\n",
      "Epoch 21 Loss: 360.15298825094146\n",
      "Epoch 22 Loss: 2382.794402396842\n",
      "Epoch 23 Loss: 542.9460882422043\n",
      "Epoch 24 Loss: 308.50129427975173\n",
      "Epoch 25 Loss: 277.3441021017832\n",
      "Epoch 26 Loss: 264.69404899910705\n",
      "Epoch 27 Loss: 258.6429685827804\n",
      "Epoch 28 Loss: 244.99498798422619\n",
      "Epoch 29 Loss: 245.98766264196945\n",
      "Epoch 30 Loss: 299.96424915365975\n",
      "Epoch 31 Loss: 3160.647018275849\n",
      "Epoch 32 Loss: 245.5950748234579\n",
      "Epoch 33 Loss: 246.16315569942944\n",
      "Epoch 34 Loss: 236.4380635823289\n",
      "Epoch 35 Loss: 240.06185416652733\n",
      "Epoch 36 Loss: 247.13829084945053\n",
      "Epoch 37 Loss: 226.75396381012382\n",
      "Epoch 38 Loss: 216.94485133967987\n",
      "Epoch 39 Loss: 214.7404137180276\n",
      "Epoch 40 Loss: 228.3484288045805\n",
      "Epoch 41 Loss: 207.1087346011645\n",
      "Epoch 42 Loss: 212.94231461825436\n",
      "Epoch 43 Loss: 323.84905676645775\n",
      "Epoch 44 Loss: 727699.9006998246\n",
      "Epoch 45 Loss: 280.23634100613526\n",
      "Epoch 46 Loss: 612.6864368752257\n",
      "Epoch 47 Loss: 350.0478078241218\n",
      "Epoch 48 Loss: 260.79058837890625\n",
      "Epoch 49 Loss: 269.37956582683404\n",
      "Epoch 50 Loss: 228.60433662101013\n",
      "Epoch 51 Loss: 229.67129218741638\n",
      "Epoch 52 Loss: 201.03581405012574\n",
      "Epoch 53 Loss: 195.2167573954961\n",
      "Epoch 54 Loss: 193.85770552125695\n",
      "Epoch 55 Loss: 188.39644055823757\n",
      "Epoch 56 Loss: 187.51669797505417\n",
      "Epoch 57 Loss: 198.37326073319946\n",
      "Epoch 58 Loss: 190.14627780653026\n",
      "Epoch 59 Loss: 189.31109250734931\n",
      "Epoch 60 Loss: 187.19180969342793\n",
      "Epoch 61 Loss: 180.31316689269184\n",
      "Epoch 62 Loss: 193.19250469991604\n",
      "Epoch 63 Loss: 215.26930200237118\n",
      "Epoch 64 Loss: 192.9730980494251\n",
      "Epoch 65 Loss: 189.37126807643943\n",
      "Epoch 66 Loss: 196.58073945241432\n",
      "Epoch 67 Loss: 187.88168457762836\n",
      "Epoch 68 Loss: 243.60630308438652\n",
      "Epoch 69 Loss: 250.27859609420986\n",
      "Epoch 70 Loss: 229.08623214617168\n",
      "Epoch 71 Loss: 213.09466267938484\n",
      "Epoch 72 Loss: 192.64484165139393\n",
      "Epoch 73 Loss: 185.68010992546604\n",
      "Epoch 74 Loss: 214.4627792149374\n",
      "Epoch 75 Loss: 190.44854158897923\n",
      "Epoch 76 Loss: 214.2748400544467\n",
      "Epoch 77 Loss: 213.35583535285846\n",
      "Epoch 78 Loss: 200.4384164940821\n",
      "Epoch 79 Loss: 204.5246693598081\n",
      "Epoch 80 Loss: 284.91953881146156\n",
      "Epoch 81 Loss: 742.387043600213\n",
      "Epoch 82 Loss: 249.3978664189169\n",
      "Epoch 83 Loss: 209.27260098392017\n",
      "Epoch 84 Loss: 215.21439202191078\n",
      "Epoch 85 Loss: 227.43050475969707\n",
      "Epoch 86 Loss: 234.5098680208807\n",
      "Epoch 87 Loss: 199.0960129254485\n",
      "Epoch 88 Loss: 180.6892523308323\n",
      "Epoch 89 Loss: 183.8174079738251\n",
      "Epoch 90 Loss: 233.5747091345591\n",
      "Epoch 91 Loss: 196.55259341409763\n",
      "Epoch 92 Loss: 256.5066707558828\n",
      "Epoch 93 Loss: 197.22059944884418\n",
      "Epoch 94 Loss: 214.22309922518795\n",
      "Epoch 95 Loss: 193.1387604752632\n",
      "Epoch 96 Loss: 201.22216762908516\n",
      "Epoch 97 Loss: 195.0305721335215\n",
      "Epoch 98 Loss: 180.25931891349896\n",
      "Epoch 99 Loss: 172.21866944718033\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        for batch_i, batch in enumerate(seq_dataloader):\n",
    "                # forward pass\n",
    "                x = batch[0].view(-1, x_dim).to(device)\n",
    "                rep_z, x_mu, x_logvar, x_rec = model(x)\n",
    "                loss = ELBO(x, x_rec,x_mu, x_logvar)\n",
    "                batch_losses.append(loss.data.cpu().item())\n",
    "                \n",
    "                model_optim.zero_grad()\n",
    "                loss.backward()\n",
    "                model_optim.step()\n",
    "        train_loss.append(sum(batch_losses)/len(batch_losses))\n",
    "        print(f\"Epoch {epoch} Loss: {train_loss[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(max_epochs=300,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=TensorBoardLogger(save_dir=\"logs/\"))\n",
    "model = LitBasicVae(latent_dim, x_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type    | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | relu           | ReLU    | 0      | train\n",
      "1 | sig            | Sigmoid | 0      | train\n",
      "2 | fc1_enc        | Linear  | 47.5 M | train\n",
      "3 | fc3_enc_mean   | Linear  | 131 K  | train\n",
      "4 | fc3_enc_logvar | Linear  | 131 K  | train\n",
      "5 | fc1_dec        | Linear  | 131 K  | train\n",
      "6 | fc3_dec        | Linear  | 47.6 M | train\n",
      "---------------------------------------------------\n",
      "95.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "95.5 M    Total params\n",
      "381.895   Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/anaconda3/envs/ProteinManifoldLearning/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df25b7264e44aec8e0d8704d05c7ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, seq_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "latent_coord = []\n",
    "model.eval()\n",
    "for batch_i, batch in enumerate(seq_dataloader):\n",
    "    # labels.append(batch[1].to('cpu').numpy())\n",
    "    # x = batch[0].view(-1, x_dim).to(device)\n",
    "    rep_z, x_mu, x_logvar, x_rec = model(batch)\n",
    "    latent_coord.append(x_mu.to('cpu').detach().numpy())\n",
    "# labels = np.concatenate(labels)\n",
    "latent_coord = np.concatenate(latent_coord, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9596222 ,  0.05223989, -0.28218463, ...,  0.3014732 ,\n",
       "         0.29052728,  0.07012247],\n",
       "       [-0.04073771,  0.04249161,  0.02327852, ...,  0.00876101,\n",
       "         0.03203313,  0.05199884],\n",
       "       [-0.0397366 ,  0.04562294,  0.01559087, ...,  0.01748188,\n",
       "         0.02926253,  0.04737437],\n",
       "       ...,\n",
       "       [-0.07724916,  0.05222613,  0.06604559, ...,  0.00254532,\n",
       "         0.0976226 ,  0.05033163],\n",
       "       [-0.03886154,  0.04329761,  0.02328013, ...,  0.00500974,\n",
       "         0.03040096,  0.05167186],\n",
       "       [-0.03613582,  0.04582736,  0.02963253, ...,  0.00195167,\n",
       "         0.04730478,  0.04262269]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = torch.pca_lowrank(torch.tensor(latent_coord), q=None, center=True, niter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data = torch.matmul(torch.tensor(latent_coord), V[:, :2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x3d61ee170>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(projected_data.numpy()[:, 0], projected_data.numpy()[:, 1], s = 2, color = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProteinManifoldLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
